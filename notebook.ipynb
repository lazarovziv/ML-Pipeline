{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 9534819,
     "sourceType": "datasetVersion",
     "datasetId": 5807251
    }
   ],
   "dockerImageVersionId": 30776,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T21:07:08.574486Z",
     "start_time": "2024-10-02T21:07:07.408053Z"
    },
    "execution": {
     "iopub.status.busy": "2024-10-02T23:25:13.210862Z",
     "iopub.execute_input": "2024-10-02T23:25:13.211261Z",
     "iopub.status.idle": "2024-10-02T23:25:13.575060Z",
     "shell.execute_reply.started": "2024-10-02T23:25:13.211209Z",
     "shell.execute_reply": "2024-10-02T23:25:13.573952Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [],
   "id": "250518b50bbd9b35"
  },
  {
   "cell_type": "code",
   "source": [
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-02T21:07:08.594262Z",
     "start_time": "2024-10-02T21:07:08.111006Z"
    },
    "execution": {
     "iopub.status.busy": "2024-10-02T23:25:13.576271Z",
     "iopub.execute_input": "2024-10-02T23:25:13.576646Z",
     "iopub.status.idle": "2024-10-02T23:25:13.581139Z",
     "shell.execute_reply.started": "2024-10-02T23:25:13.576614Z",
     "shell.execute_reply": "2024-10-02T23:25:13.580268Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [],
   "id": "b42c0d612553eb7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_data_dir = '/kaggle/input/alzheimer/alzheimer'\n",
    "train_base_dir = f'{base_data_dir}/train'\n",
    "test_base_dir = f'{base_data_dir}/test'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ebda8c5fc6b4839"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_distributions = {}\n",
    "classes = os.listdir(train_base_dir)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff54fd94576071b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for class_name in classes:\n",
    "    class_distributions[class_name] = len(os.listdir(f'{train_base_dir}/{class_name}'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc62130c22577a12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_distributions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccf663c96933f86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "418c664fcfc8233e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Distribution of classes in the train dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b68c0e75f49a2ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.bar(classes, class_distributions.values())\n",
    "plt.xticks(rotation=45)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae3fc38b257cbaf9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from PIL import Image"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a387f83e5b6b335"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_image_dimensions(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    return image.width, image.height"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb979a7fa4e19497"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec657a8e7e7c98a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_dimensions_distributions = {\n",
    "    'width': [],\n",
    "    'height': [],\n",
    "    'class': []\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98f9696d07c2954b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for class_name in classes:\n",
    "    for image_name in tqdm(os.listdir(f'{train_base_dir}/{class_name}')):\n",
    "        image_path = f'{train_base_dir}/{class_name}/{image_name}'\n",
    "        image_width, image_height = get_image_dimensions(image_path)\n",
    "        class_dimensions_distributions['width'].append(image_width)\n",
    "        class_dimensions_distributions['height'].append(image_height)\n",
    "        class_dimensions_distributions['class'].append(class_name)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a04aa7c54605ff79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dimensions_df = pd.DataFrame(class_dimensions_distributions)\n",
    "dim_df = pd.DataFrame(class_dimensions_distributions)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1baea7cce5730005"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dimensions_df.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb0022bcc1ed149f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dimensions_df.describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4703931768db6400"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Distribution of photos' dimensions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2974dfe368647fbc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dimensions_df['width'].value_counts(), dimensions_df['height'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26ec18f9d18d82b4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dimensions_df.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa9bef75bc881b03"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def image_to_numpy(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    return np.array(image)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82e1a84c91db5840"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# setting tensor to store all images' pixels\n",
    "images = np.empty((5121, 208, 176))\n",
    "class_to_idx = {}\n",
    "for i in range(len(classes)):\n",
    "    class_to_idx[classes[i]] = i\n",
    "images_labels = np.empty(5121)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42760fac8bd71c5e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image_idx = 0\n",
    "for class_name in classes:\n",
    "    for image_name in tqdm(os.listdir(f'{train_base_dir}/{class_name}')):\n",
    "        image_path = f'{train_base_dir}/{class_name}/{image_name}'\n",
    "        images[image_idx] = image_to_numpy(image_path)\n",
    "        images_labels[image_idx] = class_to_idx[class_name]\n",
    "        image_idx += 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae3f3f2fb14b53ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "467500d04373297d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Displaying heatmap for each class - whiter is weaker density"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b43a1313c2f6f19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10), sharey=True, sharex=True)\n",
    "fig.tight_layout()\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    ax = axes[i//2, i%2]\n",
    "    class_images = images[np.where(images_labels == i)]\n",
    "    heatmap = np.sum(class_images, axis=0)\n",
    "    sns.heatmap(heatmap, ax=ax, cmap='gray')\n",
    "    ax.set_title(f'Pixel Density for {classes[i]}')\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.get_xaxis().set_visible(False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b8f1c13887595d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Drop all empty pixels to reduce dimensions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41f8401d2c116e6c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get leftmost non empty pixel\n",
    "non_empty_pixels = np.where(images > 0)[1:]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f7f966e2e95f4aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "non_empty_pixels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "baddcc7032f6fa3f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "uppermost_pixel = np.min(non_empty_pixels[0])\n",
    "bottommost_pixel = np.max(non_empty_pixels[0])\n",
    "leftmost_pixel = np.min(non_empty_pixels[1])\n",
    "rightmost_pixel = np.max(non_empty_pixels[1])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53e5f3c1c7cfdf2c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# top left\n",
    "leftmost_pixel, uppermost_pixel"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c53ed7349ece5229"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# bottom right\n",
    "rightmost_pixel, bottommost_pixel"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18fd7b2f4febf6cf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dropping\n",
    "images = images[:, uppermost_pixel:bottommost_pixel+1, leftmost_pixel:rightmost_pixel+1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25ceb72c03b8ff4f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "images.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ae4d7f796344653"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(images.reshape(images.shape[0], -1))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97b14ac3dca2ef1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ea872b8978ee2bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# number of reduced dimensions\n",
    "208 * 176 - 25344"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98c1f5f478b4853a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10), sharey=True, sharex=True)\n",
    "fig.tight_layout()\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    ax = axes[i//2, i%2]\n",
    "    class_images = images[np.where(images_labels == i)]\n",
    "    heatmap = np.sum(class_images, axis=0)\n",
    "    sns.heatmap(heatmap, ax=ax, cmap='gray')\n",
    "    ax.set_title(f'Heatmap for {classes[i]}')\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.get_xaxis().set_visible(False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "569c126670c9c89a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import scipy as sp"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df590adfb5f2220d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_transformed_and_original(transformed_images):\n",
    "    # plot\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(18, 10))\n",
    "    subfigs = fig.subfigures(2, 2)\n",
    "\n",
    "    for class_idx, subfig in enumerate(subfigs.flat):\n",
    "        subfig.suptitle(classes[class_idx])\n",
    "        axes = subfig.subplots(1, 2)\n",
    "\n",
    "        blurred_images = transformed_images[np.where(images_labels == class_idx)]\n",
    "        blurred_heatmap = np.sum(blurred_images, axis=0)\n",
    "        sns.heatmap(blurred_heatmap, ax=axes[0], cmap='gray')\n",
    "        axes[0].set_title('Blurred')\n",
    "        axes[0].get_yaxis().set_visible(False)\n",
    "        axes[0].get_xaxis().set_visible(False)\n",
    "\n",
    "        original_images = images[np.where(images_labels == class_idx)]\n",
    "        original_heatmap = np.sum(original_images, axis=0)\n",
    "        sns.heatmap(original_heatmap, ax=axes[1], cmap='gray')\n",
    "        axes[1].set_title('Original')\n",
    "        axes[1].get_yaxis().set_visible(False)\n",
    "        axes[1].get_xaxis().set_visible(False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3fc1a78e4f4da696"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def apply_filter(filter_func, **kwargs):\n",
    "    filtered_images = filter_func(images, **kwargs)\n",
    "    plot_transformed_and_original(filtered_images)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d0f4e04448492e1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Applying median filter with a kernel size of 3x3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44de4c11dc5483b7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "apply_filter(sp.ndimage.median_filter, size=3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a8598e973feabdf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Applying max filter with same kernel"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f852d030da4a56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "apply_filter(sp.ndimage.maximum_filter, size=3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66586b2d174314b3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Minimum filter"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf77c448cfbe24c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "apply_filter(sp.ndimage.minimum_filter, size=3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b13b502b62f11942"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Sobel filter"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75faed59f4d9482e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sobel_images_x = sp.ndimage.sobel(images, axis=1)\n",
    "sobel_images_y = sp.ndimage.sobel(images, axis=2)\n",
    "sobel_images = np.sqrt(sobel_images_x ** 2 + sobel_images_y ** 2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cafbe1ab5236e6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_transformed_and_original(sobel_images)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb76a68413a233ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# turning off pixels smaller than their image's mean\n",
    "greater_than_mean = np.where(images > images.mean(axis=0), 0, images)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4372884d69b1116"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_transformed_and_original(greater_than_mean)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63e4901253f4dcd9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Augmentation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72e6b3848ebaaab5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Training a variational autoencoder for augmenting randomized corrupted data to create new samples\n",
    "##### Inspired from the following article: https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "397da5959bb10c6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f82d94bca76e7a2b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c34c096715f92ce6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Preparing data for PyTorch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9129513164901cff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AlzheimerDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, np_images, np_labels, transform=None, target_transform=None):\n",
    "        # self.X = torch.from_numpy(np_images).to(torch.float32)\n",
    "        self.X = torch.from_numpy(np_images).to(torch.float32).unsqueeze(dim=1) # adding the single channel\n",
    "        # self.y = torch.from_numpy(np_labels).to(torch.float32)\n",
    "        self.y = torch.from_numpy(np_labels)\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        self.len = len(self.X)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        images = self.X[idx]\n",
    "        labels = self.y[idx]\n",
    "        if self.transform:\n",
    "            images = self.transform(images)\n",
    "        if self.target_transform:\n",
    "            labels = self.target_transform(labels)\n",
    "        \n",
    "        return images, labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a0685e3c6f9ec56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "alzheimer_dataset = AlzheimerDataset(images, images_labels)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abc0b4d10e5d1031"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(alzheimer_dataset, batch_size=256, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5911dd10cfed79cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Defining encoder and decoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68b31f65b0fe60ea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, encoded_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # non parameterized functionality\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # initial layers\n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=(5, 5),\n",
    "                               stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.max_indices = None\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2), \n",
    "                                    padding=(1, 1), ceil_mode=False, return_indices=True)\n",
    "        \n",
    "        # first residual block\n",
    "        self.block = nn.Sequential(\n",
    "            # bias is false as we have a bias in the BN layer\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # down sampling\n",
    "        self.down_sample1 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 3, padding=1, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "#         self.linear = nn.Linear(in_features=6*437, out_features=encoded_dim, bias=True)\n",
    "        self.linear = nn.Linear(in_features=32*23*19, out_features=encoded_dim, bias=True)\n",
    "        \n",
    "            \n",
    "    def forward(self, X):\n",
    "        X0 = self.conv1(X)\n",
    "        X0 = self.bn1(X0)\n",
    "        X0 = self.relu(X0)\n",
    "        X0, self.max_indices = self.maxpool(X0)\n",
    "        \n",
    "        # residual block\n",
    "        Y1_ = self.block(X0)\n",
    "        Y1_1 = X0 + Y1_\n",
    "        Y2 = self.down_sample1(Y1_1)\n",
    "        Y2 = self.relu(Y2)\n",
    "        \n",
    "        Y = self.flatten(Y2)\n",
    "        Y = self.linear(Y.squeeze())\n",
    "        \n",
    "        return Y\n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7da624253ab544a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, encoded_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "#         self.linear = nn.Linear(in_features=encoded_dim, out_features=6*437, bias=True)\n",
    "        self.linear = nn.Linear(in_features=encoded_dim, out_features=32*23*19, bias=True)\n",
    "        # reshape to 6x437 before passing through this block\n",
    "        self.up_sample1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=3, padding=1, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.block = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=16, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=16, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.max_indices = None\n",
    "        self.max_unpool = nn.MaxUnpool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.conv_transpose1 = nn.ConvTranspose2d(16, 1, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(1)\n",
    "        \n",
    "        \n",
    "    def set_max_indices(self, max_indices):\n",
    "        self.max_indices = max_indices\n",
    "\n",
    "    def forward(self, X):\n",
    "        X0 = self.linear(X)\n",
    "        # reshaping to (batch, channels, pixels) shape\n",
    "        X0 = X0.reshape(-1, 32, 23, 19)\n",
    "        X0 = self.relu(X0)\n",
    "        \n",
    "        Y1 = self.up_sample1(X0)\n",
    "        Y1_1 = self.block(Y1)\n",
    "        Y2 = Y1 + Y1_1 \n",
    "        \n",
    "        Y = self.max_unpool(Y2, self.max_indices)\n",
    "        Y = self.conv_transpose1(Y)\n",
    "        Y = self.bn1(Y)\n",
    "        Y = self.relu(Y)\n",
    "        # remove last row and column in the image\n",
    "        Y = Y[..., :-1, :-1]\n",
    "        # print(Y.shape)\n",
    "        \n",
    "        return Y"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "337d5c5d7d5839a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, encoded_dim):\n",
    "        super().__init__()\n",
    "        # latent space is 2, i.e: encoding to a 2d space\n",
    "        self.encoder = ConvEncoder(1, encoded_dim)\n",
    "        self.decoder = ConvDecoder(encoded_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        x = self.encoder(X)\n",
    "        self.decoder.set_max_indices(self.encoder.max_indices)\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88ec3173c3771b8d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "autoencoder = AutoEncoder(176)\n",
    "encoder = autoencoder.encoder\n",
    "decoder = autoencoder.decoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "474ab07dada35272"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sum(p.numel() for p in autoencoder.parameters())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27ca2579b5f82fc0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=lr, betas=(0.99, 0.99))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "loss = nn.MSELoss()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e333d7add201c6ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "autoencoder.to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b166060ab5bebeb5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 75\n",
    "losses = torch.zeros(epochs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f3cc665f23f41c1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "    autoencoder.train()\n",
    "    for batch_idx, (batch_X, _) in enumerate(dataloader):\n",
    "        batch_X = batch_X.to(device=device)\n",
    "        optimizer.zero_grad()\n",
    "        batch_output = autoencoder(batch_X)\n",
    "        batch_loss = loss(batch_output, batch_X)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        # print('Evaluating model...')\n",
    "        autoencoder.eval()\n",
    "        output = autoencoder(alzheimer_dataset.X.to(device))\n",
    "        output_loss = loss(output, alzheimer_dataset.X.to(device))\n",
    "        losses[epoch] = output_loss.item()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7b356434727587d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(losses.cpu())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21635ec41ae6f9f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "maximum_images = sp.ndimage.maximum_filter(images, size=3)\n",
    "single_channel_images = np.expand_dims(images, axis=1)\n",
    "single_channel_images.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fef88a774fd12ec6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# getting a generated image from the original\n",
    "with torch.no_grad():\n",
    "    # expanding the channel\n",
    "    original_batched_image = np.expand_dims(images[1], axis=0)\n",
    "    # expanding batch\n",
    "    original_batched_image = np.expand_dims(original_batched_image, axis=0)\n",
    "    generated_image = autoencoder(torch.from_numpy(original_batched_image) \\\n",
    "                                  # .flatten(start_dim=1) \\\n",
    "                                  .to(device=device, dtype=torch.float32)) \\\n",
    "                                  .flatten(start_dim=0, end_dim=2)\n",
    "    print(generated_image.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23251747ebc171f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 10), sharey=True, sharex=True)\n",
    "fig.tight_layout()\n",
    "\n",
    "axes[0].set_title('Original')\n",
    "axes[0].imshow(images[0], cmap='gray')\n",
    "axes[1].set_title('Generated')\n",
    "axes[1].imshow(generated_image.cpu(), cmap='gray')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b2db144ae73637c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    encoded_images = encoder(torch.from_numpy(single_channel_images).to(device=device, dtype=torch.float32)).cpu()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "945e7e0ab0fa110d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoded_images.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b08991e47a81f1e3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.scatter(encoded_images[:, 0], encoded_images[:, 1], c=images_labels)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bac0480d609d2d23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    plt.imshow(decoder(torch.tensor([[[0, 500]]]).to(device, torch.float32).detach()).cpu().squeeze().numpy(), cmap='gray')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4b2af2980aca3a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('test')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64593c44aa46213"
  }
 ]
}
